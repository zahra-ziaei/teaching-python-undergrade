{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 4 (solutions).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahra-ziaei/teaching-python-undergrade/blob/main/Homework_4_(solutions).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Backpropagation for bias vectors (1 point)\n",
        "\n",
        "In class, we discussed a multilayer perceptron (neural network) whose layers were all \"dense\", i.e. the output $a^m \\in \\mathbb{R}^{N^m}$ of the $m$th layer is computed as \n",
        "\\begin{align*}\n",
        "z^m &= W^m a^{m - 1} + b^m \\\\\n",
        "a^m &= \\sigma^m(z^m)\n",
        "\\end{align*}\n",
        "where $W^m \\in \\mathbb{R}^{N^m \\times N^{m - 1}}$ is the weight matrix, $b^m \\in \\mathbb{R}^{N^m}$ is the bias vector, and $\\sigma^m$ is the nonlinearity. We showed that \n",
        "$$\\frac{\\partial C}{\\partial W^m} = \\frac{\\partial C}{\\partial z^m} a^{m - 1 \\top}$$\n",
        "Show that\n",
        "$$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}$$\n",
        "Hint: The derivation is almost the same as for $W$."
      ],
      "metadata": {
        "id": "nkK2eaK1SiNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution\n",
        "\n",
        "From the chain rule,\n",
        "$$\\frac{\\partial C}{\\partial b^m_i} = \\sum_{k = 1}^{N^m} \\frac{\\partial C}{\\partial z^m_k} \\frac{\\partial z^m_k}{\\partial b^m_i}$$\n",
        "All terms of this sum except the one where $k = i$ will be zero, so we have\n",
        "\\begin{align*}\n",
        "\\frac{\\partial z^m_i}{\\partial b^m_i} &= \\frac{\\partial}{\\partial b^m_i}\\left(\\sum_{l = 1}^{N^m} W^m_{il} a_l^{m - 1} + b^m_i\\right) \\\\\n",
        "&= 1, \\\\\n",
        "\\rightarrow \\frac{\\partial z^m_i}{\\partial b^m_i} &= \\begin{cases}\n",
        "0 & k \\ne i \\\\\n",
        "1 & k = i\n",
        "\\end{cases}\n",
        "\\end{align*}\n",
        "\n",
        "The original summation therefore simplifies to\n",
        "$$\\frac{\\partial C}{\\partial b^m_i} = \\frac{\\partial C}{\\partial z^m_i}$$"
      ],
      "metadata": {
        "id": "cBW9wHsfJWd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. MLP from scratch (3 points)\n",
        "\n",
        "Using numpy only, implement backward pass or a sigmoid MLP. Specifically, you will need to implement this functionality in the `train` function in the `SigmoidMLP` class below. You should write numpy code to populate the two lists `weight_gradients` and `bias_gradient`, where each entry in each list corresponds to the gradient for a weight matrix or bias vector for each layer."
      ],
      "metadata": {
        "id": "TerrZtS6T31p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, inputs, outputs):\n",
        "        # Initialize weight matrix and bias vector\n",
        "        # Getting the initialization right can be tricky, but for this problem\n",
        "        # simply drawing from a standard normal distribution should work.\n",
        "        self.weights = np.random.randn(outputs, inputs)\n",
        "        self.biases = np.random.randn(outputs, 1)\n",
        "    def __call__(self, X):\n",
        "        # Compute \\sigmoid(Wx + b)\n",
        "        return 1/(1 + np.exp(-(self.weights.dot(X) + self.biases)))\n",
        "\n",
        "class SigmoidMLP:\n",
        "\n",
        "    def __init__(self, layer_widths):\n",
        "        self.layers = []\n",
        "        for inputs, outputs in zip(layer_widths[:-1], layer_widths[1:]):\n",
        "            self.layers.append(Layer(inputs, outputs))\n",
        "    \n",
        "    def train(self, inputs, targets, learning_rate):\n",
        "        # Forward pass - compute each layer's output and store it for later use\n",
        "        layer_outputs = [inputs]\n",
        "        for layer in self.layers:\n",
        "            layer_outputs.append(layer(layer_outputs[-1]))\n",
        "        \n",
        "        # Implement backward pass to populate weight_gradients and bias_gradients\n",
        "        # lists here\n",
        "        cost_partials = [layer_outputs[-1] - targets]\n",
        "        for layer, layer_output in zip(reversed(self.layers), reversed(layer_outputs[:-1])):\n",
        "            cost_partials.append(layer.weights.T.dot(cost_partials[-1])*layer_output*(1 - layer_output))\n",
        "        cost_partials.reverse()\n",
        "        \n",
        "        # Compute weight gradient step\n",
        "        weight_gradients = []\n",
        "        for cost_partial, layer_output in zip(cost_partials[1:], layer_outputs[:-1]):\n",
        "            weight_gradients.append(cost_partial.dot(layer_output.T)/inputs.shape[1])\n",
        "        # and biases\n",
        "        bias_gradients = [cost_partial.mean(axis=1).reshape(-1, 1) for cost_partial in cost_partials[1:]]\n",
        "        \n",
        "        # Perform gradient descent by applying updates\n",
        "        for weight_gradient, bias_gradient, layer in zip(weight_gradients, bias_gradients, self.layers):\n",
        "            layer.weights -= weight_gradient*learning_rate\n",
        "            layer.biases -= bias_gradient*learning_rate\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        a = inputs\n",
        "        for layer in self.layers:\n",
        "            a = layer(a)\n",
        "        return a\n",
        "\n",
        "def train_mlp(n_iterations, learning_rate):\n",
        "    mlp = SigmoidMLP([2, 2, 1])\n",
        "    inputs = np.array([[0, 1, 0, 1], \n",
        "                       [0, 0, 1, 1]])\n",
        "    targets = np.array([[0, 1, 1, 0]])\n",
        "    for _ in range(int(1e3)):\n",
        "        mlp.train(inputs, targets, learning_rate)\n",
        "    return mlp"
      ],
      "metadata": {
        "id": "W-v0EeRsV8m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You may need to change the n_iterations and learning_rate values\n",
        "# but these worked for me\n",
        "mlp = train_mlp(1000, 1.)\n",
        "# The following calls should result in (approximately) 0, 1, 1, 0\n",
        "# If the outputs are somewhat close, your training has succeeded!\n",
        "print(mlp(np.array([0, 0]).reshape(-1, 1)))\n",
        "print(mlp(np.array([0, 1]).reshape(-1, 1)))\n",
        "print(mlp(np.array([1, 0]).reshape(-1, 1)))\n",
        "print(mlp(np.array([1, 1]).reshape(-1, 1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoAl7lW_WyNg",
        "outputId": "e0168f6f-2d8f-4fbf-dc67-0f67a7463840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.01468419]]\n",
            "[[0.98093589]]\n",
            "[[0.98608727]]\n",
            "[[0.01255408]]\n"
          ]
        }
      ]
    }
  ]
}